{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import os, glob\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/jooyoungson/P2_fashion_classifier/PyTorch_Ver/dataset_pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_num = glob.glob(base_dir + '/train/**')\n",
    "len(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_num = glob.glob(base_dir + '/val/**')\n",
    "len(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) #img Tensor로 변경후 normalize\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=base_dir +'/train', \n",
    "                                              transform = trans )\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=base_dir + '/val', transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()        \n",
    "        #Layer1\n",
    "        # CONV\n",
    "            # input_channel_개수: 3\n",
    "            # output_channel_개수(필터의 개수): 64\n",
    "            # kernel_size(필터의 사이즈): 3 X 3\n",
    "            # padding = 1\n",
    "        # ReLU\n",
    "        # MaxPool\n",
    "            # kernel_size = 2\n",
    "            # stride = 2\n",
    "        #    Input           -> (1, 3, 224, 224)\n",
    "        #    layer1.Conv     -> (1, 64, 224, 224)\n",
    "        #    layer1.Pool     -> (1, 64, 112, 112)\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 64, 3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #Layer2\n",
    "        # CONV\n",
    "            # input_channel_개수: 64\n",
    "            # output_channel_개수(필터의 개수): 32\n",
    "            # kernel_size(필터의 사이즈): 3 X 3\n",
    "            # padding = 1\n",
    "        # ReLU\n",
    "        # MaxPool\n",
    "            # kernel_size = 2\n",
    "            # stride = 2\n",
    "        #    Input           -> (2, 64, 112, 112)\n",
    "        #    layer2.Conv     -> (2, 32, 112, 112)\n",
    "        #    layer2.Pool     -> (2, 32, 56, 56)\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.Conv2d(64, 32,  3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        #Layer3\n",
    "        # CONV\n",
    "            # input_channel_개수: 32\n",
    "            # output_channel_개수(필터의 개수): 10\n",
    "            # kernel_size(필터의 사이즈): 4 X 4\n",
    "            # padding = 1\n",
    "        # ReLU\n",
    "        # MaxPool\n",
    "            # kernel_size = 2\n",
    "            # stride = 2\n",
    "        #    Input           -> (2, 32, 56, 56)\n",
    "        #    layer2.Conv     -> (2, 10, 55, 55)\n",
    "        #    layer2.Pool     -> (2, 10, 27, 27)\n",
    "        self.layer3 = nn.Sequential(\n",
    "                        nn.Conv2d(32, 10, 4, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2, padding=1))\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "                        nn.Conv2d(10, 5, 2),\n",
    "                        nn.Conv2d(5, 1, 2),\n",
    "                        nn.MaxPool2d(kernel_size=2, padding=1))\n",
    "        \n",
    "        # L4 FC 14 * 14 inputs -> 196 outputs\n",
    "        self.fc1 = torch.nn.Linear(196, 39, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.view(-1, 196)   # Flatten them for FC\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 2\n",
    "batch_size = 1\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/PIL/Image.py:968: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  ' expressed in bytes should be converted ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 3.06505609\n",
      "[Epoch:    2] cost = 2.38756919\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "total_batch = len(train_loader)\n",
    "model.train()    # set the model to train mode (dropout=True)\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in train_loader:\n",
    "\n",
    "        X = X\n",
    "        Y = Y\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/Users/jooyoungson/Datasets/Fashion'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs\n",
    "                labels = labels\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        str_epoch = str(epoch)\n",
    "        torch.save(model.state_dict(), data_dir+\"model-\"+str_epoch+\".pth\") # Save checkpoint\n",
    "\n",
    "        input_tensor = torch.rand(1, 3, 224, 224) #an example input\n",
    "        script_module = torch.jit.trace(model, input_tensor)\n",
    "        script_module.save(data_dir+\"class1.pt\")\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.mobilenet_v2(pretrained=True)\n",
    "# backbone = model_ft.features\n",
    "# new_model = nn.Sequential(backbone, nn.AdaptiveAvgPool2d(1000), nn.Linear(1000,39))\n",
    "\n",
    "model_ft.classifier[1].out_features = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TransferLearning을 위한 가중치 고정\n",
    "for param in model_ft.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 2\n",
    "batch_size = 1\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/PIL/Image.py:968: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  ' expressed in bytes should be converted ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 4.8707 Acc: 0.0607\n",
      "val Loss: 3.4300 Acc: 0.2467\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/PIL/Image.py:968: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  ' expressed in bytes should be converted ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.3024 Acc: 0.1877\n",
      "val Loss: 2.2022 Acc: 0.4167\n",
      "Training complete in 6m 50s\n",
      "Best val Acc: 0.416667\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
